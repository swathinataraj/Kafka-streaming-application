{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-62a90ac48320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Run stream for 10 minutes just in case no detection of producer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;31m# ssc.awaitTermination()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopSparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopGraceFully\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import statements\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import geohash as geh\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "#Function that sends data to mongodb of each batch every 10 seconds\n",
    "def sendDataToDB(iter):\n",
    "    client = MongoClient()\n",
    "    db = client.FIT5148 #Specifying the name of the database to create in mongo db\n",
    "    week11 = db.Streaming_Data #Specifyting the name of the collection to create\n",
    "    \n",
    "    #Creating 3 lists to store climate, aqua and terra data separately\n",
    "    climate_data = []\n",
    "    aqua_data = []\n",
    "    terra_data = []\n",
    "    \n",
    "    #For every record in each iteration(from each batch), do the following:\n",
    "    for record in iter:\n",
    "        data = record[1] #store the useful record\n",
    "        my1 = json.loads(data)\n",
    "        var1 = geh.encode(float(my1['latitude']),float(my1['longitude']), precision = 5) #calculate geohash with precision = 5\n",
    "        my1['geohash'] = var1 #Insert geohash value into the dict with a new key\n",
    "        #using sender id we determine which record belongs to which data and append to respective list\n",
    "        if my1['sender_id'] == 'id_1':\n",
    "            climate_data.append(my1)\n",
    "        elif my1['sender_id'] =='id_2':\n",
    "            aqua_data.append(my1)\n",
    "        else:\n",
    "            terra_data.append(my1)\n",
    "    \n",
    "    a = len(aqua_data)\n",
    "    t = len(terra_data)\n",
    "    if t == 1 and a == 1: #If aqua and terra data exist (There can be at most 1 record from aqua and terra each)\n",
    "        for c in climate_data:\n",
    "            g1 = c['geohash']  \n",
    "            my_arr = []\n",
    "            my_dict = c\n",
    "            for i in aqua_data:\n",
    "                for j in terra_data:\n",
    "                    id1 = i['geohash']\n",
    "                    id2 = j['geohash']  \n",
    "                    if id1 == id2: #If geo hash of aqua and terra match, we calculate the averages\n",
    "                        d1 = dict()\n",
    "                        sur1 = i['surface_temperature_celcius']\n",
    "                        sur2 = j['surface_temperature_celcius']\n",
    "                        con1 = i['confidence']\n",
    "                        con2 = j['confidence']\n",
    "                        avg_sur = (sur1 + sur2) / 2.0\n",
    "                        avg_con = (con1 + con2) / 2.0\n",
    "                        d1['average_surface_temp'] = avg_sur\n",
    "                        d1['average_confidence'] = avg_con\n",
    "                        my_arr.append(d1)\n",
    "                        if g1 == id1: #append list with average values if there exists a nearest climate data\n",
    "                            my_dict['hotspot_data'] = my_arr\n",
    "                    else: #if aqua and terra geohashes don't match\n",
    "                        if g1 == id1:\n",
    "                            my_arr.append(i)\n",
    "                            my_dict['hotspot_data'] = my_arr #add aqua data if it matches geohash of climate data\n",
    "                        elif g1 == id2:\n",
    "                            my_arr.append(j)\n",
    "                            my_dict['hotspot_data'] = my_arr #add terra data if it matches geohash of climate data\n",
    "                        else:\n",
    "                            print(\"Just climate data\")\n",
    "\n",
    "    elif t == 1 and a != 1: #if aqua data doesn't exist\n",
    "        for i in climate_data:\n",
    "            my_arr = []\n",
    "            my_dict = i\n",
    "            for j in terra_data: \n",
    "                id1 = i['geohash']\n",
    "                id2 = j['geohash']\n",
    "                if id1 == id2: #check if the geohash match with the climate data and add terra info\n",
    "                    my_arr.append(j)\n",
    "                    my_dict['hotspot_data'] = my_arr            \n",
    "                else:\n",
    "                    print(\"Just climate data\")\n",
    "                                \n",
    "    elif a == 1 and t != 1: #if terra data doesn't exist\n",
    "        for i in climate_data:\n",
    "            my_arr = []\n",
    "            my_dict = i\n",
    "            for j in aqua_data:\n",
    "                id1 = i['geohash']\n",
    "                id2 = j['geohash']\n",
    "                if id1 == id2: #check if aqua data location is closer to climate data location and add aqua info\n",
    "                    my_arr.append(j)\n",
    "                    my_dict['hotspot_data'] = my_arr\n",
    "                else:\n",
    "                    print(\"Just climate data\")\n",
    "    else: \n",
    "        print(\"Just climate data\")\n",
    "    for rec in climate_data: #insert records from climate data\n",
    "        week11.insert(rec)        \n",
    "        \n",
    "    #print(\"--------------The end-------------------------\")\n",
    "    client.close()\n",
    "\n",
    "n_secs = 10 #Batch interval of 10 seconds\n",
    "topic = \"streaming02\" #Name of the topic from producers 1, 2 and 3\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\") #2 Master threads\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week11-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
